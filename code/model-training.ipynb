{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2833c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection, Trainer, TrainingArguments\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396321d",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754b27ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# MacOS MPS (Metal Performance Shaders) support\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# CUDA support\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load the pre-trained DETR model and processor\n",
    "# Note: The model is downloaded from Hugging Face Hub, so make sure you have internet access\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").to(device)\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "id2label = {0: \"Button\", 1: \"CheckBox\", \n",
    "            2: \"ComboBox\", 3: \"Heading\", \n",
    "            4: \"Image\", 5: \"Label\", \n",
    "            6: \"Link\", 7: \"Paragraph\", \n",
    "            8: \"RadioButton\" , 9: \"TextBox\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model.config.id2label = id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b462ee",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24d8c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "class DetrCocoDataset(Dataset):\n",
    "    def __init__(self, root, annFile, processor: DetrImageProcessor):\n",
    "        self.ds = CocoDetection(root, annFile)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, raw_anns = self.ds[idx]\n",
    "\n",
    "        # 1) fix raw_anns in-place: map any string â†’ int\n",
    "        for ann in raw_anns:\n",
    "            if isinstance(ann[\"category_id\"], str):\n",
    "                ann[\"category_id\"] = label2id[ann[\"category_id\"]]\n",
    "\n",
    "        # 2) build the full COCO dict\n",
    "        target = {\n",
    "            \"image_id\": raw_anns[0][\"image_id\"],\n",
    "            \"annotations\": raw_anns,\n",
    "        }\n",
    "\n",
    "        return {\"image\": image, \"target\": target}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images     = [item[\"image\"] for item in batch]\n",
    "    targets    = [item[\"target\"] for item in batch]  # full COCO dicts\n",
    "    \n",
    "    encoding = processor(\n",
    "      images=images,\n",
    "      annotations=targets,\n",
    "      return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "# Instantiate\n",
    "train_dataset = DetrCocoDataset(\"../data/sketch2code-data/train\", \"../data/sketch2code-data/annotations/instances_train.json\", processor)\n",
    "val_dataset   = DetrCocoDataset(\"../data/sketch2code-data/val\",   \"../data/sketch2code-data/annotations/instances_val.json\",   processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd0f971",
   "metadata": {},
   "source": [
    "Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7284e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\", # If you use colab, set this to \"/content/checkpoints\"\n",
    "\n",
    "    # batches\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "\n",
    "    # epochs\n",
    "    num_train_epochs=100, # personal experimentation shows sketch2code data converges in 70-100 epochs\n",
    "    \n",
    "    dataloader_num_workers=2,  # Or try 4\n",
    "    dataloader_pin_memory=True,\n",
    "\n",
    "   # evaluation & checkpointing (I use it with wandb.ai to track training) then use ./checkpoints for outputdir\n",
    "    do_eval=True,    # run validation periodically\n",
    "    eval_strategy=\"epoch\",  # evaluate every eval_steps\n",
    "    save_strategy=\"epoch\",  # save every save_steps\n",
    "    save_total_limit=6,\n",
    "    load_best_model_at_end=True,    # keep checkpoint with best eval loss\n",
    "\n",
    "    # logging\n",
    "    logging_steps=max(13, 1),  # twice per epoch\n",
    "\n",
    "    # optimizer\n",
    "    learning_rate=5e-5,             # slightly lower LR for small data\n",
    "    weight_decay=1e-4,\n",
    "\n",
    "    # mixed precision\n",
    "    fp16=device.type == \"cuda\",  # use fp16 if available\n",
    "    bf16=True, # Comment this line if you are not using MPS (MacOS)\n",
    "\n",
    "    # pass through all fields\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,  # pack up our pixel_values + labels\n",
    ")  # Trainer accepts any PyTorch dataset that returns dicts\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd72dd",
   "metadata": {},
   "source": [
    "Testing (Loss) ~ 0.9679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea03b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: {'test_loss': 0.9679349660873413, 'test_model_preparation_time': 0.0015, 'test_runtime': 13.6349, 'test_samples_per_second': 1.687, 'test_steps_per_second': 0.22}\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Load the model\n",
    "best_model = DetrForObjectDetection.from_pretrained(\"./checkpoints/checkpoint-3172\").to(device)\n",
    "\n",
    "# Load COCO annotations\n",
    "coco_test = COCO(\"../data/sketch2code-data/annotations/instances_test.json\")\n",
    "\n",
    "# Create test dataset with correct argument order\n",
    "test_dataset = DetrCocoDataset(\"../data/sketch2code-data/test\", \"../data/sketch2code-data/annotations/instances_test.json\", processor)\n",
    "\n",
    "# Create test loader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Create evaluation-specific training arguments\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./test_results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=False,  # Disable for MPS\n",
    "    report_to=\"none\",\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# Create trainer for evaluation\n",
    "tester = Trainer(\n",
    "    model=best_model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "test_results = tester.evaluate(\n",
    "    eval_dataset=test_dataset, \n",
    "    metric_key_prefix=\"test\"\n",
    ")\n",
    "\n",
    "print(\"Test results:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71387627",
   "metadata": {},
   "source": [
    "Helpers (for visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cffef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: convert [x,y,w,h] â†’ [x_min,y_min,x_max,y_max]\n",
    "# Your function is good. torchvision.ops.box_convert can also do this.\n",
    "def box_xywh_to_xyxy(boxes: torch.Tensor) -> torch.Tensor:\n",
    "    if boxes.ndim == 1: # Handle single box case for safety\n",
    "        boxes = boxes.unsqueeze(0)\n",
    "    if boxes.shape[0] == 0: # Handle empty tensor\n",
    "        return torch.empty((0,4), dtype=boxes.dtype, device=boxes.device)\n",
    "    x, y, w, h = boxes.unbind(-1)\n",
    "    return torch.stack((x, y, x + w, y + h), dim=-1)\n",
    "\n",
    "def iou_match(pred_boxes_cls, gt_boxes_cls, iou_threshold):\n",
    "    num_preds = pred_boxes_cls.shape[0]\n",
    "    num_gts = gt_boxes_cls.shape[0]\n",
    "    \n",
    "    if num_preds == 0 and num_gts == 0: return [], [], []\n",
    "    if num_preds == 0: return [], [], list(range(num_gts))\n",
    "    if num_gts == 0: return [], list(range(num_preds)), []\n",
    "    \n",
    "    iou_matrix = torchvision.ops.box_iou(pred_boxes_cls, gt_boxes_cls)\n",
    "    matches = []\n",
    "    matched_gts = set()\n",
    "    \n",
    "    # For each prediction (ideally sorted by confidence), find best available GT\n",
    "    for pred_idx in range(num_preds):\n",
    "        # Find best IoU with unmatched GTs\n",
    "        best_iou = -1\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for gt_idx in range(num_gts):\n",
    "            if gt_idx not in matched_gts and iou_matrix[pred_idx, gt_idx] > best_iou:\n",
    "                best_iou = iou_matrix[pred_idx, gt_idx]\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        # Match if above threshold\n",
    "        if best_iou >= iou_threshold:\n",
    "            matches.append((pred_idx, best_gt_idx))\n",
    "            matched_gts.add(best_gt_idx)\n",
    "    \n",
    "    # Calculate FP and FN\n",
    "    matched_preds = {m[0] for m in matches}\n",
    "    fp_indices = [i for i in range(num_preds) if i not in matched_preds]\n",
    "    fn_indices = [i for i in range(num_gts) if i not in matched_gts]\n",
    "    \n",
    "    return matches, fp_indices, fn_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4d15669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/sbwqylbn39g821_1qy6vjnjr0000gn/T/ipykernel_11408/1792042333.py:7: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap        = cm.get_cmap(\"tab20\", num_classes if num_classes <= 20 else 20) # tab20 has 20 colors (incase we choose to have more classes)\n"
     ]
    }
   ],
   "source": [
    "## Configuration\n",
    "best_model  = best_model.to(device).eval()\n",
    "threshold   = 0.5 # Detection confidence threshold\n",
    "iou_thresh  = 0.5 # IoU threshold for matching\n",
    "max_viz     = 8\n",
    "num_classes = len(id2label)\n",
    "cmap        = cm.get_cmap(\"tab20\", num_classes if num_classes <= 20 else 20) # tab20 has 20 colors (incase we choose to have more classes)\n",
    "label_colors= {cid: cmap(cid % cmap.N) for cid in range(num_classes)} # Use modulo for >20 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5dbfdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision=0.7638, Recall=0.9174, F1-Score=0.8335\n",
      "TP: 333, FP: 103, FN: 30\n"
     ]
    }
   ],
   "source": [
    "# Metrics accumulators\n",
    "total_tp_overall = 0\n",
    "total_fp_overall = 0\n",
    "total_fn_overall = 0\n",
    "viz_candidates = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch_from_loader in enumerate(test_loader):\n",
    "        # Move to device (only tensors)\n",
    "        # Assuming batch_from_loader is a dict of tensors or contains tensors\n",
    "        # If batch_from_loader contains non-tensor data like list of dicts for labels, handle carefully\n",
    "        # For this example, assume pixel_values and pixel_mask are the primary tensors from loader.\n",
    "        \n",
    "        pixel_values_batch = batch_from_loader[\"pixel_values\"].to(device)\n",
    "        # Handle optional pixel_mask\n",
    "        pixel_mask_batch = batch_from_loader.get(\"pixel_mask\")\n",
    "        if pixel_mask_batch is not None:\n",
    "            pixel_mask_batch = pixel_mask_batch.to(device)\n",
    "\n",
    "        inputs = {\"pixel_values\": pixel_values_batch}\n",
    "        if pixel_mask_batch is not None:\n",
    "            inputs[\"pixel_mask\"] = pixel_mask_batch\n",
    "            \n",
    "        outputs = best_model(**inputs)\n",
    "\n",
    "        # --- CRITICAL CHANGE: Determine original image sizes for post-processing ---\n",
    "        original_image_sizes_list = []\n",
    "        current_batch_size = pixel_values_batch.shape[0]\n",
    "        for i_in_batch in range(current_batch_size):\n",
    "            dataset_idx = batch_idx * test_loader.batch_size + i_in_batch\n",
    "            if dataset_idx < len(test_dataset):\n",
    "                # Assuming test_dataset[dataset_idx][\"image\"] is a PIL Image\n",
    "                pil_image = test_dataset[dataset_idx][\"image\"]\n",
    "                original_w, original_h = pil_image.size # PIL size is (width, height)\n",
    "                original_image_sizes_list.append(torch.tensor([original_h, original_w], device=device)) # Target size is (height, width)\n",
    "            else:\n",
    "                # Should not happen if dataloader iterates correctly over the dataset\n",
    "                # Fallback, but this is likely problematic as it uses resized dimensions\n",
    "                # (Lucas note) My current implmentation does not hit this case, but it's good to have a fallback\n",
    "                pv_h, pv_w = pixel_values_batch.shape[2:]\n",
    "                original_image_sizes_list.append(torch.tensor([pv_h, pv_w], device=device))\n",
    "                print(f\"Warning: Dataset index {dataset_idx} out of bounds. Using fallback size for postprocessing.\")\n",
    "\n",
    "        target_sizes_for_postprocessing = torch.stack(original_image_sizes_list)\n",
    "\n",
    "        # Postâ€‘process with original target_sizes\n",
    "        results_batch = processor.post_process_object_detection(\n",
    "            outputs, target_sizes=target_sizes_for_postprocessing, threshold=threshold\n",
    "        )\n",
    "\n",
    "        # Perâ€‘image matching\n",
    "        for i_in_batch, det_per_image in enumerate(results_batch):\n",
    "            dataset_idx = batch_idx * test_loader.batch_size + i_in_batch\n",
    "            if dataset_idx >= len(test_dataset): continue # Safety break\n",
    "\n",
    "            sample_from_dataset = test_dataset[dataset_idx]\n",
    "\n",
    "            # Ground truth (these are in original image coordinates)\n",
    "            ann       = sample_from_dataset[\"target\"][\"annotations\"]\n",
    "            if not ann: # Handle images with no GT annotations\n",
    "                gt_boxes_xyxy  = torch.empty((0, 4), device=device, dtype=torch.float)\n",
    "                gt_labels = torch.empty((0,), device=device, dtype=torch.long)\n",
    "            else:\n",
    "                gt_boxes_xywh  = torch.tensor([a[\"bbox\"] for a in ann], device=device, dtype=torch.float)\n",
    "                gt_boxes_xyxy  = box_xywh_to_xyxy(gt_boxes_xywh) # Your converter\n",
    "                gt_labels = torch.tensor([a[\"category_id\"] for a in ann], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "            # Predictions (now scaled to original image dimensions by post_process_object_detection)\n",
    "            pred_boxes_xyxy  = det_per_image[\"boxes\"].to(device)\n",
    "            pred_labels = det_per_image[\"labels\"].to(device)\n",
    "            # pred_scores = det_per_image[\"scores\"].to(device) # If you want to use scores\n",
    "\n",
    "            # --- Match by IoU per class (Revised TP/FP/FN logic for clarity) ---\n",
    "            # These will store indices relative to pred_boxes_xyxy and gt_boxes_xyxy for the current image\n",
    "            current_img_tp_pred_indices = []\n",
    "            \n",
    "            num_preds_img = pred_boxes_xyxy.shape[0]\n",
    "            num_gts_img = gt_boxes_xyxy.shape[0]\n",
    "\n",
    "            # Keep track of which predictions and GTs have been matched as TPs\n",
    "            # This prevents a single prediction/GT from being part of multiple TPs\n",
    "            # or a TP being later counted as FP/FN.\n",
    "            pred_matched_as_tp = [False] * num_preds_img\n",
    "            gt_matched_as_tp = [False] * num_gts_img\n",
    "            \n",
    "            # Iterate over all unique classes present in either predictions or ground truths for this image\n",
    "            unique_classes_on_img = torch.unique(torch.cat((gt_labels, pred_labels)))\n",
    "\n",
    "            for cls_id in unique_classes_on_img:\n",
    "                # Get predictions and GTs for the current class\n",
    "                pred_indices_cls_mask = (pred_labels == cls_id)\n",
    "                gt_indices_cls_mask = (gt_labels == cls_id)\n",
    "\n",
    "                current_pred_boxes_cls = pred_boxes_xyxy[pred_indices_cls_mask]\n",
    "                current_gt_boxes_cls = gt_boxes_xyxy[gt_indices_cls_mask]\n",
    "\n",
    "                # Get the original indices (relative to full pred_boxes_xyxy/gt_boxes_xyxy)\n",
    "                # for the items selected for this class.\n",
    "                pred_original_indices_cls = pred_indices_cls_mask.nonzero(as_tuple=True)[0]\n",
    "                gt_original_indices_cls = gt_indices_cls_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "                if current_pred_boxes_cls.numel() == 0 and current_gt_boxes_cls.numel() == 0:\n",
    "                    continue\n",
    "                # Inside the loop `for cls_id in unique_classes_on_img:`\n",
    "                # `matches_cls` contains pairs of (idx_in_current_pred_boxes_cls, idx_in_current_gt_boxes_cls)\n",
    "                matches_cls, _, _ = iou_match(current_pred_boxes_cls, current_gt_boxes_cls, iou_thresh)\n",
    "                \n",
    "                for pred_cls_idx, gt_cls_idx in matches_cls:\n",
    "                    original_pred_idx = pred_original_indices_cls[pred_cls_idx].item()\n",
    "                    original_gt_idx = gt_original_indices_cls[gt_cls_idx].item()\n",
    "\n",
    "                    # Ensure this pred-GT pair hasn't been matched already (e.g., if classes overlap somehow, though unlikely here)\n",
    "                    # And that this specific pred or gt hasn't been used in another TP for a *different* class if that were possible\n",
    "                    # For standard per-class matching, this check is more about ensuring one-to-one matching within a class context if iou_match allows multiple.\n",
    "                    if not pred_matched_as_tp[original_pred_idx] and not gt_matched_as_tp[original_gt_idx]:\n",
    "                        current_img_tp_pred_indices.append(original_pred_idx)\n",
    "                        pred_matched_as_tp[original_pred_idx] = True\n",
    "                        gt_matched_as_tp[original_gt_idx] = True\n",
    "            \n",
    "            # False Positives: Predictions not marked as TPs\n",
    "            current_img_fp_pred_indices = [i for i, matched in enumerate(pred_matched_as_tp) if not matched]\n",
    "            # False Negatives: Ground truths not marked as TPs\n",
    "            current_img_fn_gt_indices = [i for i, matched in enumerate(gt_matched_as_tp) if not matched]\n",
    "\n",
    "            total_tp_overall += len(current_img_tp_pred_indices)\n",
    "            total_fp_overall += len(current_img_fp_pred_indices)\n",
    "            total_fn_overall += len(current_img_fn_gt_indices)\n",
    "\n",
    "            # Save for visualization if any errors occurred\n",
    "            if current_img_fp_pred_indices or current_img_fn_gt_indices:\n",
    "                viz_candidates.append({\n",
    "                    \"img\":         sample_from_dataset[\"image\"], # Original PIL image\n",
    "                    \"pred_boxes\":  pred_boxes_xyxy.cpu(),  # Now in original image coords\n",
    "                    \"pred_labels\": pred_labels.cpu(),\n",
    "                    # \"pred_scores\": det_per_image[\"scores\"].cpu(), # Optional\n",
    "                    \"tp_idx\":      current_img_tp_pred_indices, # Indices of pred_boxes that are TPs\n",
    "                    \"fp_idx\":      current_img_fp_pred_indices, # Indices of pred_boxes that are FPs\n",
    "                    \"gt_boxes\":    gt_boxes_xyxy.cpu(),    # Original GT boxes\n",
    "                    \"gt_labels\":   gt_labels.cpu(),\n",
    "                    \"fn_idx\":      current_img_fn_gt_indices,   # Indices of gt_boxes that are FNs\n",
    "                })\n",
    "\n",
    "# Print overall precision & recall\n",
    "precision = total_tp_overall / (total_tp_overall + total_fp_overall + 1e-8)\n",
    "recall    = total_tp_overall / (total_tp_overall + total_fn_overall + 1e-8)\n",
    "f1_score  = 2 * (precision * recall) / (precision + recall + 1e-8) if (precision + recall > 0) else 0.0\n",
    "\n",
    "print(f\"Precision={precision:.4f}, Recall={recall:.4f}, F1-Score={f1_score:.4f}\")\n",
    "print(f\"TP: {total_tp_overall}, FP: {total_fp_overall}, FN: {total_fn_overall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d037dd",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee33b0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving visualizations to: /Users/yunseolee/Documents/main/GitHub/Personal/dinov2-personalized-federated-learning/code/visualizations_output\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "viz_output_dir = \"visualizations_output\"\n",
    "os.makedirs(viz_output_dir, exist_ok=True)\n",
    "print(f\"Saving visualizations to: {os.path.abspath(viz_output_dir)}\")\n",
    "\n",
    "if viz_candidates:\n",
    "    samples_to_visualize = random.sample(viz_candidates, min(max_viz, len(viz_candidates)))\n",
    "else:\n",
    "    samples_to_visualize = []\n",
    "    print(\"No candidates with errors to visualize.\")\n",
    "\n",
    "for idx, sample_data in enumerate(samples_to_visualize):\n",
    "    img_pil = sample_data[\"img\"].convert(\"RGB\")\n",
    "    \n",
    "    # Predictions are in sample_data[\"pred_boxes\"], sample_data[\"pred_labels\"]\n",
    "    # Ground truths are in sample_data[\"gt_boxes\"], sample_data[\"gt_labels\"]\n",
    "    \n",
    "    # Indices for TP boxes are in sample_data[\"tp_idx\"] (these are indices into pred_boxes)\n",
    "    # Indices for FP boxes are in sample_data[\"fp_idx\"] (these are indices into pred_boxes)\n",
    "    # Indices for FN boxes are in sample_data[\"fn_idx\"] (these are indices into gt_boxes)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10)) # Larger figure for clarity\n",
    "    ax.imshow(img_pil)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Visualization {idx+1}\")\n",
    "\n",
    "    # True Positives (solid green outline, from predicted boxes)\n",
    "    for i in sample_data[\"tp_idx\"]:\n",
    "        cls_id = sample_data[\"pred_labels\"][i].item()\n",
    "        label  = id2label.get(cls_id, f\"CLS {cls_id}\")\n",
    "        # score = sample_data[\"pred_scores\"][i].item() # If you stored scores\n",
    "        color  = label_colors.get(cls_id, (0.5,0.5,0.5)) # Default gray\n",
    "        x1,y1,x2,y2 = sample_data[\"pred_boxes\"][i]\n",
    "        \n",
    "        ax.add_patch(plt.Rectangle((x1,y1), x2-x1, y2-y1,\n",
    "                                   edgecolor='green', facecolor=\"none\", # Specific color for TP\n",
    "                                   linewidth=2, linestyle=\"-\"))\n",
    "        ax.text(x1, y1-3, f\"{label} (TP)\", # {score:.2f}\n",
    "                fontsize=7, color=\"white\",\n",
    "                bbox=dict(facecolor='green', alpha=0.6, pad=1, edgecolor='none'))\n",
    "\n",
    "    # False Positives (dashed red outline, from predicted boxes)\n",
    "    for i in sample_data[\"fp_idx\"]:\n",
    "        cls_id = sample_data[\"pred_labels\"][i].item()\n",
    "        label  = id2label.get(cls_id, f\"CLS {cls_id}\")\n",
    "        # score = sample_data[\"pred_scores\"][i].item() # If you stored scores\n",
    "        color  = label_colors.get(cls_id, (0.5,0.5,0.5))\n",
    "        x1,y1,x2,y2 = sample_data[\"pred_boxes\"][i]\n",
    "        \n",
    "        ax.add_patch(plt.Rectangle((x1,y1), x2-x1, y2-y1,\n",
    "                                   edgecolor='red', facecolor=\"none\", # Specific color for FP\n",
    "                                   linewidth=2, linestyle=\"--\"))\n",
    "        ax.text(x1, y1-3, f\"{label} (FP)\", # {score:.2f}\n",
    "                fontsize=7, color=\"white\",\n",
    "                bbox=dict(facecolor='red', alpha=0.6, pad=1, edgecolor='none'))\n",
    "\n",
    "    # False Negatives (dotted orange outline, from ground truth boxes)\n",
    "    for i in sample_data[\"fn_idx\"]:\n",
    "        cls_id = sample_data[\"gt_labels\"][i].item()\n",
    "        label  = id2label.get(cls_id, f\"CLS {cls_id}\")\n",
    "        color  = label_colors.get(cls_id, (0.5,0.5,0.5))\n",
    "        x1,y1,x2,y2 = sample_data[\"gt_boxes\"][i]\n",
    "        \n",
    "        ax.add_patch(plt.Rectangle((x1,y1), x2-x1, y2-y1,\n",
    "                                   edgecolor='orange', facecolor=\"none\",# Specific color for FN\n",
    "                                   linewidth=2, linestyle=\":\"))\n",
    "        ax.text(x1, y1-3, f\"{label} (FN)\",\n",
    "                fontsize=7, color=\"black\", # Contrast for orange\n",
    "                bbox=dict(facecolor='orange', alpha=0.6, pad=1, edgecolor='none'))\n",
    "\n",
    "    # plt.show() # This will pop up a window for each image\n",
    "    plt.savefig(os.path.join(viz_output_dir, f\"visualization_{idx}.png\"), bbox_inches=\"tight\")\n",
    "    plt.close(fig) # Close the figure to free memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
